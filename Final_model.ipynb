{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirsalhuv/MassDetector/blob/main/Final_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryZnpp9nw2g3",
        "outputId": "de0e9b9f-4658-4581-d097-21c3cb1d0cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4kVWMnp9rsX"
      },
      "source": [
        "## 1.2 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijsMtVYX9o7l"
      },
      "outputs": [],
      "source": [
        "# For image manipulation\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Image loading saving, copy ,measuring time\n",
        "import time, json, os\n",
        "import datetime\n",
        "import pickle\n",
        "# For plots\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn import BCELoss\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models import vgg16_bn, VGG16_BN_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJBenbek92QR"
      },
      "source": [
        "# 1.3 Definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfrgs1859zGo"
      },
      "outputs": [],
      "source": [
        "# Amir Salhuv:\n",
        "dataset_dir = '/content/drive/MyDrive/Project_OpenU/CBIS-DDSM/'\n",
        "\n",
        "# Yonatan Salhuv:\n",
        "#dataset_dir = '/content/drive/MyDrive/'\n",
        "\n",
        "training_folder = dataset_dir + \"Full_model/\"\n",
        "models_folder = training_folder+'Models/'\n",
        "data_folder = dataset_dir + 'Latest_model_data/'\n",
        "run_mode = \"evaluate_model\"\n",
        "\n",
        "# Classes and batch size\n",
        "n_classes=2\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - models definitions"
      ],
      "metadata": {
        "id": "dSFcNlHk1bln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Clone YOLOv5 repository\n",
        "  !git clone -q https://github.com/ultralytics/yolov5.git\n",
        "\n",
        "  # for running the train.py from the yolov5 directory\n",
        "  %cd yolov5\n",
        "\n",
        "  # Install dependencies\n",
        "  !pip install -q -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_a4Gmxr0_OM",
        "outputId": "aebbf678-32c7-49b6-9684-fa8741e01762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5/yolov5/yolov5/yolov5/yolov5/yolov5/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  def double_conv(in_channels, out_channels):\n",
        "      return nn.Sequential(\n",
        "          nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(inplace=True),\n",
        "          nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "          nn.BatchNorm2d(out_channels),\n",
        "          nn.ReLU(inplace=True)\n",
        "      )\n",
        "\n",
        "\n",
        "  def up_conv(in_channels, out_channels):\n",
        "      return nn.ConvTranspose2d(\n",
        "          in_channels, out_channels, kernel_size=2, stride=2\n",
        "      )\n",
        "\n",
        "\n",
        "  class VGGUnet(nn.Module):\n",
        "    #  VGG-16 (with BN) encoder.\n",
        "\n",
        "      def __init__(self, encoder, *, pretrained=False, out_channels=1):\n",
        "          super().__init__()\n",
        "\n",
        "          self.encoder = encoder(pretrained=pretrained).features\n",
        "          self.block1 = nn.Sequential(*self.encoder[:6])\n",
        "          self.block2 = nn.Sequential(*self.encoder[6:13])\n",
        "          self.block3 = nn.Sequential(*self.encoder[13:20])\n",
        "          self.block4 = nn.Sequential(*self.encoder[20:27])\n",
        "          self.block5 = nn.Sequential(*self.encoder[27:34])\n",
        "\n",
        "          self.bottleneck = nn.Sequential(*self.encoder[34:])\n",
        "          self.conv_bottleneck = double_conv(512, 1024)\n",
        "\n",
        "          self.up_conv6 = up_conv(1024, 512)\n",
        "          self.conv6 = double_conv(512 + 512, 512)\n",
        "          self.up_conv7 = up_conv(512, 256)\n",
        "          self.conv7 = double_conv(256 + 512, 256)\n",
        "          self.up_conv8 = up_conv(256, 128)\n",
        "          self.conv8 = double_conv(128 + 256, 128)\n",
        "          self.up_conv9 = up_conv(128, 64)\n",
        "          self.conv9 = double_conv(64 + 128, 64)\n",
        "          self.up_conv10 = up_conv(64, 32)\n",
        "          self.conv10 = double_conv(32 + 64, 32)\n",
        "          self.conv11 = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "\n",
        "      def forward(self, x):\n",
        "          block1 = self.block1(x)\n",
        "          block2 = self.block2(block1)\n",
        "          block3 = self.block3(block2)\n",
        "          block4 = self.block4(block3)\n",
        "          block5 = self.block5(block4)\n",
        "\n",
        "          bottleneck = self.bottleneck(block5)\n",
        "          x = self.conv_bottleneck(bottleneck)\n",
        "\n",
        "          x = self.up_conv6(x)\n",
        "          x = torch.cat([x, block5], dim=1)\n",
        "          x = self.conv6(x)\n",
        "\n",
        "          x = self.up_conv7(x)\n",
        "          x = torch.cat([x, block4], dim=1)\n",
        "          x = self.conv7(x)\n",
        "\n",
        "          x = self.up_conv8(x)\n",
        "          x = torch.cat([x, block3], dim=1)\n",
        "          x = self.conv8(x)\n",
        "\n",
        "          x = self.up_conv9(x)\n",
        "          x = torch.cat([x, block2], dim=1)\n",
        "          x = self.conv9(x)\n",
        "\n",
        "          x = self.up_conv10(x)\n",
        "          x = torch.cat([x, block1], dim=1)\n",
        "          x = self.conv10(x)\n",
        "\n",
        "          x = self.conv11(x)\n",
        "\n",
        "          x = torch.sigmoid(x)\n",
        "\n",
        "          return x"
      ],
      "metadata": {
        "id": "sC7E7GmH1aLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDiH7pHHYY5T"
      },
      "source": [
        "# 2- Import the models and input data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Load Yolo model ######\n",
        "weights_path = models_folder + 'yolo.pt'\n",
        "yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path=weights_path,force_reload=True)\n",
        "\n",
        "##### Load Patch model  ######\n",
        "patch_model = VGGUnet(models.vgg16_bn, pretrained=True, out_channels=1)\n",
        "\n",
        "# Load the model weights\n",
        "patch_model.load_state_dict(torch.load(models_folder + 'Patch_model_padding_2_classes.pth',map_location=torch.device('cpu')))\n",
        "\n",
        "##### input data #####\n",
        "X_train = np.load(data_folder + 'X_train.npy')\n",
        "y_train = np.load(data_folder + 'y_train.npy')\n",
        "X_val = np.load(data_folder + 'X_val.npy')\n",
        "y_val = np.load(data_folder + 'y_val.npy')"
      ],
      "metadata": {
        "id": "UUEhslCESRR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57244541-d5f3-4c29-db98-efae72dbdc9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 ðŸš€ 2023-7-20 Python-3.10.6 torch-2.0.1+cu118 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5x summary: 322 layers, 86173414 parameters, 0 gradients, 203.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqRN-W823ZOQ"
      },
      "source": [
        "## 5.1 Define consolidated model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedModel(nn.Module):\n",
        "    def __init__(self, yolo_model, patch_model):\n",
        "        super().__init__()\n",
        "        self.yolo_model = yolo_model.eval()\n",
        "        self.patch_model = patch_model.eval()\n",
        "        self.test_image_transforms = transforms.Compose([\n",
        "              transforms.Resize((224, 224)),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "          ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        predicted_mask  = np.zeros((input.shape[0],input.shape[1]),dtype=np.uint8)\n",
        "\n",
        "        with torch.no_grad():  # Disables gradient calculation to save memory\n",
        "            yolo_out = self.yolo_model(input)\n",
        "\n",
        "            # Get all bouding boxes\n",
        "            for *box, _, _ in yolo_out.pred[0]:\n",
        "              # Get the bounding box (and normilize them)\n",
        "              x, y, w, h = (int(box[0]),int(box[1]),int(box[2]),int(box[3]))\n",
        "\n",
        "              # Cut the input according to the window\n",
        "              patch = input[y:h,x:w,:]\n",
        "\n",
        "              # Resize according to the input for the 2nd model\n",
        "              resized_patch = cv2.resize(patch,(256,256))\n",
        "\n",
        "              # Convert the numpy array to a PIL Image\n",
        "              output = Image.fromarray(resized_patch.astype('uint8'))\n",
        "\n",
        "              # Convert to patch input shape\n",
        "              output = self.test_image_transforms(output)\n",
        "              output = torch.unsqueeze(output,0)\n",
        "\n",
        "              # Inference from patch model\n",
        "              output = self.patch_model(output)\n",
        "\n",
        "\n",
        "              # Revert back to the mask size\n",
        "              output = torch.squeeze(output)\n",
        "              output = (output.cpu().numpy()>0.5)\n",
        "              output = output.astype(np.uint8)\n",
        "\n",
        "              # return back to original size\n",
        "              output = cv2.resize(output,(w-x,h-y))\n",
        "\n",
        "              # Add the output to the total detected mask\n",
        "              predicted_mask[y:h,x:w]= output\n",
        "\n",
        "\n",
        "            '''\n",
        "            fig, ax = plt.subplots(2, 3, figsize=(6, 6))\n",
        "            ax = ax.flatten()\n",
        "            ax[0].imshow(patch)\n",
        "            ax[0].title.set_text('patch')\n",
        "            ax[1].imshow(input,cmap = 'gray')\n",
        "            ax[1].title.set_text('input')\n",
        "            ax[2].imshow(resized_patch,cmap = 'gray')\n",
        "            ax[2].title.set_text('resized_patch')\n",
        "            ax[3].imshow(targets,cmap = 'gray')\n",
        "            ax[3].title.set_text('GT')\n",
        "            ax[4].imshow(predicted_mask,cmap = 'gray')\n",
        "            ax[4].title.set_text('predicted_mask')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            '''\n",
        "\n",
        "        return predicted_mask\n"
      ],
      "metadata": {
        "id": "ozPPayhAkALl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKJz08xq3g63"
      },
      "source": [
        "## 5.3 Metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_score(output, target, smooth=1e-6):\n",
        "  output = output.cpu().numpy() > 0.5\n",
        "  target = target.cpu().numpy() > 0.5\n",
        "  intersection = (output & target).sum((1,2,3))\n",
        "  union = (output | target).sum((1,2,3))\n",
        "  dice = (2. * intersection + smooth) / (output.sum((1,2,3)) + target.sum((1,2,3)) + smooth)\n",
        "  return dice\n",
        "\n",
        "def iou_score(output, target):\n",
        "  smooth = 1e-6\n",
        "  #if torch.is_tensor(output):\n",
        "  #    output = torch.sigmoid(output).data.cpu().numpy()\n",
        "  #if torch.is_tensor(target):\n",
        "  #    target = target.data.cpu().numpy()\n",
        "  output = (output > 0.5)*1\n",
        "  target = 1*(target > 0.5)\n",
        "  intersection = (output & target).sum()\n",
        "  union = (output | target).sum()\n",
        "  return (intersection + smooth) / (union + smooth),intersection"
      ],
      "metadata": {
        "id": "enHJm2AzyevL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KOvO_0P08Q9"
      },
      "source": [
        "## 5.5 Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYa-yW2F14g7"
      },
      "source": [
        "### 5.5.2 evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spOFvYiz169_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49acb422-b510-4932-9f1f-5023110b60ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on device=cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "209it [09:29,  2.73s/it]\n"
          ]
        }
      ],
      "source": [
        "if run_mode == 'evaluate_model':\n",
        "  %matplotlib inline\n",
        "  # Images location\n",
        "  val_image_dir = data_folder +'images/val'\n",
        "  val_mask_dir = data_folder + 'masks/val'\n",
        "  val_list = sorted(os.listdir(val_image_dir))\n",
        "\n",
        "\n",
        "  # Specify device\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  # Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
        "  print(f'Working on device={device}')\n",
        "\n",
        "  # Create the model and transfer to device\n",
        "  model = CombinedModel(yolo_model,patch_model)\n",
        "  model.to(device)\n",
        "\n",
        "  iou_scores = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for i,image in tqdm(enumerate(val_list[:])):\n",
        "          # Read and prepare the mask for the model output\n",
        "          target = cv2.imread(os.path.join(val_mask_dir, image), cv2.IMREAD_GRAYSCALE)* 255\n",
        "\n",
        "          # Read the image and send to the model\n",
        "          img = cv2.imread(os.path.join(val_image_dir, image))\n",
        "          output = model(img)\n",
        "\n",
        "          iou,intersection = iou_score(output, target)\n",
        "          iou_scores.append(iou)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iou_scores = np.array(iou_scores)\n",
        "iou2 = iou_scores[iou_scores>0.1]\n",
        "print(f'Average IoU score on validation set: {(np.mean(iou_scores))}, iou2: {np.mean(iou2)}')\n"
      ],
      "metadata": {
        "id": "VwQOdksCI0BT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(np.array(iou_scores)<0.1)/len(val_list)*100)"
      ],
      "metadata": {
        "id": "F1f95menI81Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(iou_scores)"
      ],
      "metadata": {
        "id": "8vK7WCUNUxAj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}