{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirsalhuv/MassDetector/blob/main/pytorch_model_YOLOv8Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryZnpp9nw2g3",
        "outputId": "492fd26f-0187-496a-b6d2-7f6d782437ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics  comet_ml"
      ],
      "metadata": {
        "id": "sILiHHWjVEKQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a7ff14-fb62-418a-f356-efc27a1c7e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m599.6/599.6 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m534.7/534.7 kB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.1/510.1 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4kVWMnp9rsX"
      },
      "source": [
        "## 1.2 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijsMtVYX9o7l"
      },
      "outputs": [],
      "source": [
        "# For image manipulation\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Image loading saving, copy ,measuring time\n",
        "import time, json, os\n",
        "import datetime\n",
        "import pickle\n",
        "# For plots\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn import BCELoss\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJBenbek92QR"
      },
      "source": [
        "# 1.3 Definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfrgs1859zGo"
      },
      "outputs": [],
      "source": [
        "# Amir Salhuv:\n",
        "dataset_dir = '/content/drive/MyDrive/Project_OpenU/CBIS-DDSM/manifest/'\n",
        "\n",
        "# Yonatan Salhuv:\n",
        "#dataset_dir = '/content/drive/MyDrive/'\n",
        "\n",
        "training_folder = dataset_dir + \"Sparse_data_for_model/\"\n",
        "run_mode = \"evaluate_model\"\n",
        "\n",
        "# Yolo training\n",
        "yolo_base_folder = '/content/drive/MyDrive/Project_OpenU/CBIS-DDSM/Latest_model_data/'\n",
        "yolo_training_foder = yolo_base_folder + 'Images/train'\n",
        "yolo_validation_foder = yolo_base_folder + 'Images/val'\n",
        "\n",
        "\n",
        "# Classes and batch size\n",
        "n_classes=2\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split to train and validation folder"
      ],
      "metadata": {
        "id": "bpLzESfrVS2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode == \"create_yolo_train_test_split\": # todo - needs update for Yolo folders shape (done manually)\n",
        "  # directories\n",
        "  img_dir = training_folder+'Images_new/'\n",
        "  mask_dir = training_folder+'Masks_new/'\n",
        "\n",
        "  # Images\n",
        "  images_path = sorted(os.listdir(img_dir))\n",
        "  masks_path = sorted(os.listdir(mask_dir))\n",
        "\n",
        "  # Assuming X is your data and y are your labels.\n",
        "  X_train, X_test, y_train, y_test = train_test_split(images_path, masks_path, test_size=0.2, random_state=42)\n",
        "\n",
        "  for filename in tqdm(images_path):\n",
        "    # If in training set, put in the training folder\n",
        "    if filename in X_train:\n",
        "      shutil.copy(img_dir + filename, yolo_training_foder +'Images/' + filename)\n",
        "      shutil.copy(mask_dir + filename, yolo_training_foder + 'Masks/' + filename)\n",
        "    else:\n",
        "      shutil.copy(img_dir + filename, yolo_validation_foder +'Images/' + filename)\n",
        "      shutil.copy(mask_dir + filename, yolo_validation_foder + 'Masks/' + filename)"
      ],
      "metadata": {
        "id": "tJCmywdcYw-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Yolo folder"
      ],
      "metadata": {
        "id": "qX2jO3eHVO5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode == \"create_yolo_folder\": # todo - needs update for Yolo folders shape (done manually)\n",
        "\n",
        "  # directories:\n",
        "  # Training\n",
        "  training_img_dir = yolo_training_foder +'Images'\n",
        "  training_mask_dir = yolo_training_foder +'Masks'\n",
        "  training_output_dir = yolo_training_foder +'Labels'\n",
        "  # ensure output directory exists\n",
        "  os.makedirs(training_output_dir, exist_ok=True)\n",
        "\n",
        "  # Validation\n",
        "  validation_img_dir = yolo_validation_foder +'Images'\n",
        "  validation_mask_dir = yolo_validation_foder +'Masks'\n",
        "  validation_output_dir = yolo_validation_foder +'Labels'\n",
        "  # ensure output directory exists\n",
        "  os.makedirs(validation_output_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "  def convert_to_yolo_format(mask_image,mask_filename):\n",
        "      # find contours in the mask\n",
        "      contours, _ = cv2.findContours(mask_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "      # Filter contours based on area\n",
        "      min_contour_area = 100  # Minimum area to be considered a valid contour\n",
        "      contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_contour_area]\n",
        "      if len(contours)>1:\n",
        "        print(f\"\\nmask bigger than 1: {mask_filename}, number of contours {len(contours)}\")\n",
        "      elif len(contours)==0:\n",
        "        print(f\"\\nMask error{mask_filename}\")\n",
        "\n",
        "      yolo_annotations = []\n",
        "      for contour in contours:\n",
        "          # compute bounding box\n",
        "          x, y, w, h = cv2.boundingRect(contour)\n",
        "          # convert to yolo format\n",
        "          x_center = (x + w / 2) / mask_image.shape[1]\n",
        "          y_center = (y + h / 2) / mask_image.shape[0]\n",
        "          width = w / mask_image.shape[1]\n",
        "          height = h / mask_image.shape[0]\n",
        "          # object class is 0 as we assume we only have one class\n",
        "          yolo_annotations.append(f'0 {x_center} {y_center} {width} {height}')\n",
        "\n",
        "      return yolo_annotations\n",
        "\n",
        "    # Training folder\n",
        "  for filename in tqdm(sorted(os.listdir(training_img_dir))):\n",
        "      # read the image and mask\n",
        "      mask = cv2.imread(os.path.join(training_mask_dir, filename), cv2.IMREAD_GRAYSCALE)* 255\n",
        "\n",
        "      # convert mask to yolo format\n",
        "      yolo_annotations = convert_to_yolo_format(mask,filename)\n",
        "\n",
        "      # write annotations to file\n",
        "      with open(os.path.join(training_output_dir, filename.replace('.png', '.txt')), 'w') as f:\n",
        "          for annotation in yolo_annotations:\n",
        "              f.write(annotation + '\\n')\n",
        "\n",
        "  # Validation folder\n",
        "  for filename in tqdm(sorted(os.listdir(validation_img_dir))):\n",
        "      # read the image and mask\n",
        "      mask = cv2.imread(os.path.join(validation_mask_dir, filename), cv2.IMREAD_GRAYSCALE)* 255\n",
        "\n",
        "      # convert mask to yolo format\n",
        "      yolo_annotations = convert_to_yolo_format(mask,filename)\n",
        "\n",
        "      # write annotations to file\n",
        "      with open(os.path.join(validation_output_dir, filename.replace('.png', '.txt')), 'w') as f:\n",
        "          for annotation in yolo_annotations:\n",
        "              f.write(annotation + '\\n')\n"
      ],
      "metadata": {
        "id": "8NN4m09IN8X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwWcfqyh14gT"
      },
      "source": [
        "# 5 - Training the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode==\"training_model\" or run_mode==\"evaluate_model\":\n",
        "  # Clone YOLOv5 repository\n",
        "  !git clone -q https://github.com/ultralytics/yolov5.git\n",
        "\n",
        "  # for running the train.py from the yolov5 directory\n",
        "  %cd yolov5\n",
        "\n",
        "  # Install dependencies\n",
        "  !pip install -q -r requirements.txt"
      ],
      "metadata": {
        "id": "r28lhwQ8bzjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7310d239-794d-4138-ff29-c1c8ec900fa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode==\"training_model\":\n",
        "  # This is an example data configuration. Replace with your own.\n",
        "  # Assume that this file is stored in the current directory.\n",
        "  train_dir = 'images/train'\n",
        "  val_dir = 'images/val'\n",
        "  num_classes = 1\n",
        "  class_names = ['mass']  # Replace with your actual class names\n",
        "\n",
        "  data_yaml = \"\"\"\n",
        "  path: {} # dataset root dir\n",
        "  train: {}\n",
        "  val: {}\n",
        "  nc: {}  # number of classes\n",
        "  names: {}  # class names\n",
        "  \"\"\".format(yolo_base_folder,train_dir, val_dir, num_classes, class_names)\n",
        "\n",
        "  with open(\"data.yaml\", \"w\") as file:\n",
        "      file.write(data_yaml)\n",
        "\n",
        "  !python train.py --img 640 --batch 16 --epochs 100 --data data.yaml --cfg yolov5x.yaml --weights yolov5x.pt --project training_folder+\"yolov5_models\"\n"
      ],
      "metadata": {
        "id": "IQdrDGzTaDl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqRN-W823ZOQ"
      },
      "source": [
        "## 5.1 Define the Unet block"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2.1 Option 1 - iou loss"
      ],
      "metadata": {
        "id": "G6CpJFMJ92DD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScNvxT4Z2ATp"
      },
      "outputs": [],
      "source": [
        "if run_mode == 'training_model':\n",
        "\n",
        "  def iou_loss(pred, target):\n",
        "      smooth = 1.0\n",
        "      print(f'shapes: pred {pred.shape} target {target.shape}')\n",
        "      iflat = pred.view(-1)\n",
        "      tflat = target.view(-1)\n",
        "      intersection = (iflat * tflat).sum()\n",
        "      return 1 - ((intersection + smooth) / (iflat.sum() + tflat.sum() - intersection + smooth))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imp-a0jL3XSM"
      },
      "source": [
        "# 5.4 Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode == 'evaluate_model':\n",
        "\n",
        "  iou = []\n",
        "  epsilon = 10e-6\n",
        "  # Load model\n",
        "  path_or_model = training_folder + 'yolov5_models/exp2/weights/best.pt'\n",
        "  model = torch.hub.load('ultralytics/yolov5', 'custom', path=path_or_model)\n",
        "\n",
        "  # mask path\n",
        "  mask_base_path = yolo_base_folder + '/masks/Validation/Masks_val/'\n",
        "\n",
        "    # If using a GPU\n",
        "  if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "  model.eval()"
      ],
      "metadata": {
        "id": "hrAh6yclKyC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "620a2080-15a1-4d4e-81eb-e0299474c1fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 ðŸš€ 2023-6-12 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/requests-2.27.1.dist-info/METADATA'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fusing layers... \n",
            "YOLOv5x summary: 322 layers, 86173414 parameters, 0 gradients, 203.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode == 'evaluate_model':\n",
        "\n",
        "  val_image_dir = yolo_base_folder +'images/val'\n",
        "  val_list = os.listdir(val_image_dir)\n",
        "  # Steps:\n",
        "  # 1. take an image\n",
        "  # 2. inference from image\n",
        "  # 3. draw a masks from the results\n",
        "  # 5. Compute the IOU\n",
        "  # 6. if iou >10% consider as good detection\n",
        "  for i,image_path in enumerate(val_list):\n",
        "\n",
        "    # step #1 - read the image and mask\n",
        "    img = cv2.imread(os.path.join(val_image_dir, image_path))\n",
        "    mask = cv2.imread(mask_base_path + image_path,cv2.IMREAD_GRAYSCALE)* 255\n",
        "    predicted_rect= np.zeros_like(mask)\n",
        "\n",
        "    # Step #2 -inference from image\n",
        "    results = model(os.path.join(val_image_dir, image_path))\n",
        "\n",
        "    # step #3- draw masks from result\n",
        "    for *box, _, _ in results.pred[0]:\n",
        "      # Convert box format from [center_x, center_y, width, height] to [x1, y1, x2, y2]\n",
        "      x, y, box_width, box_height = box\n",
        "      x1 = int(x)\n",
        "      y1 = int(y)\n",
        "      x2 = int(box_width)\n",
        "      y2 = int(box_height)\n",
        "\n",
        "      # Draw rectangle (full)\n",
        "      cv2.rectangle(predicted_rect, (x1, y1), (x2, y2), (255, 255, 255), -1)\n",
        "\n",
        "    # Step #4 - IOU of mask and image\n",
        "    # first - convert to shape of w,h\n",
        "    #predicted_rect = cv2.cvtColor(predicted_rect,cv2.COLOR_BGR2GRAY)\n",
        "    intersection = np.sum(predicted_rect * mask)\n",
        "    union = np.sum(predicted_rect)+np.sum(mask)\n",
        "\n",
        "    iou_temp = intersection/(union+epsilon)\n",
        "\n",
        "    # Summary of the results - add the IOU results list\n",
        "    iou.append(iou_temp*1000)\n",
        "    #print(f\" iou: {iou[i]*1000}, intesection: {intersection}, union : {union}\")"
      ],
      "metadata": {
        "id": "aibKu-jeqfOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " if run_mode == 'evaluate_model':\n",
        "    total = np.sum(np.array(iou)>0)/len(iou)*100\n",
        "    misses = len(iou)-np.sum(np.array(iou)>0)\n",
        "    print(f\"number of misdetection: {misses}, misdetection rate: {100-total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "gdplt2OlSKlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7484ce01-3009-4add-d2e8-f56d6f080a6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of misdetection: 19, misdetection rate: 9.09%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " if run_mode == 'evaluate_model':\n",
        "\n",
        "  import cv2\n",
        "  import matplotlib.pyplot as plt\n",
        "  #%matplotlib inline\n",
        "\n",
        "  # Load image\n",
        "  image_path = yolo_base_folder + 'images/val/1042.png'\n",
        "  img = cv2.imread(image_path)\n",
        "\n",
        "  # Get image dimensions\n",
        "  height, width = img.shape[:2]\n",
        "\n",
        "  # Run inference\n",
        "  results = model(image_path)\n",
        "\n",
        "  # Class labels (replace with your actual labels)\n",
        "  class_labels = ['mass']\n",
        "\n",
        "  # Loop over each prediction\n",
        "  for *box, confidence, class_id in results.pred[0]:\n",
        "      # Convert box format from [center_x, center_y, width, height] to [x1, y1, x2, y2]\n",
        "      x, y, box_width, box_height = box\n",
        "      print(box)\n",
        "      x1 = int(x)\n",
        "      y1 = int(y)\n",
        "      x2 = int(box_width)\n",
        "      y2 = int(box_height)\n",
        "\n",
        "      # Get class label\n",
        "      class_label = class_labels[int(class_id)]\n",
        "\n",
        "      # Draw rectangle and label on image\n",
        "      cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "      cv2.putText(img, f'{class_label} {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "\n",
        "  # Create a figure and axis\n",
        "  fig, ax = plt.subplots(1,figsize=(10,10))\n",
        "\n",
        "  # Display the image\n",
        "  ax.imshow(img)\n",
        "  # Show the figure with the bounding boxes\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ceHwxdupuYf0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c741c00c-009f-424c-db10-1e356552df9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(55.89012, device='cuda:0'), tensor(3018.40063, device='cuda:0'), tensor(435.42810, device='cuda:0'), tensor(3375.81348, device='cuda:0')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQrXZk055BTF",
        "outputId": "906ad284-0aa5-4f13-dcc3-7c04e3a54b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[5.58901e+01, 3.01840e+03, 4.35428e+02, 3.37581e+03, 6.61184e-01, 0.00000e+00]], device='cuda:0')]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}