{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirsalhuv/MassDetector/blob/main/Classfication_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryZnpp9nw2g3",
        "outputId": "52070c5f-aecd-4898-ea9e-1bc11d8e5515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4kVWMnp9rsX"
      },
      "source": [
        "## 1.2 Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ijsMtVYX9o7l"
      },
      "outputs": [],
      "source": [
        "# For image manipulation\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Image loading saving, copy ,measuring time\n",
        "import time, json, os\n",
        "import datetime\n",
        "import pickle\n",
        "# For plots\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.nn import BCELoss\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from torchvision import transforms, datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchvision.models import vgg16_bn, VGG16_BN_Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJBenbek92QR"
      },
      "source": [
        "# 1.3 Definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Kfrgs1859zGo"
      },
      "outputs": [],
      "source": [
        "# Amir Salhuv:\n",
        "dataset_dir = '/content/drive/MyDrive/Project_OpenU/CBIS-DDSM/'\n",
        "\n",
        "# Yonatan Salhuv:\n",
        "#dataset_dir = '/content/drive/MyDrive/'\n",
        "\n",
        "data_folder = dataset_dir + 'Latest_model_data/'\n",
        "patches_folder = data_folder + 'Patches/'\n",
        "models_folder = dataset_dir + 'Full_model/Models/'\n",
        "run_mode = \"training_classifier_model\"\n",
        "\n",
        "# Classes and batch size\n",
        "n_classes=2\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDiH7pHHYY5T"
      },
      "source": [
        "# 3- Prepare the data for the model train\n",
        "1. Cut the relevant patch according to the mask bounding boxes\n",
        "2. store it locally (for as an .npy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Validation pathces"
      ],
      "metadata": {
        "id": "OAAiY3OTwzsk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode == \"cut_pathces\":\n",
        "\n",
        "  # Open the lookup table for classification\n",
        "  lookup_table = pd.read_csv(dataset_dir+'Full_model/lookup_table.csv')\n",
        "\n",
        "  # Define the minimal ROI\n",
        "  min_roi_size = 100\n",
        "\n",
        "  # Padding size\n",
        "  pad = 200\n",
        "\n",
        "  # Create an empty list for validation set\n",
        "  X_val = list()\n",
        "  y_val = list()\n",
        "\n",
        "  # For visulazing the image and mask compared to origin, and lesion type\n",
        "  x_origin = list()\n",
        "  y_origin = list()\n",
        "  val_lesion_type = []\n",
        "\n",
        "  # Starting with validation images:\n",
        "  val_image_dir = data_folder +'images/val'\n",
        "  val_mask_dir = data_folder + 'masks/val'\n",
        "  val_list = sorted(os.listdir(val_image_dir))\n",
        "\n",
        "  # Steps:\n",
        "  # 1. take an image\n",
        "  # 2. Cut a window around the mask boundries\n",
        "  # 3. append to whole list\n",
        "\n",
        "  for i,image_path in tqdm(enumerate(val_list)):\n",
        "\n",
        "    # step #1 - read the image and mask\n",
        "    img = cv2.imread(os.path.join(val_image_dir, image_path))\n",
        "\n",
        "    # Assesing the correct masks (0=benign,1=malignant)\n",
        "    lesion_type_index = lookup_table.loc[lookup_table['image_id']== image_path]['pathology']\n",
        "    if lesion_type_index.iloc[0] == 'MALIGNANT':\n",
        "      lesion_type = 1\n",
        "    else:\n",
        "      lesion_type = 0\n",
        "\n",
        "    mask = cv2.imread(os.path.join(val_mask_dir, image_path),cv2.IMREAD_GRAYSCALE)*255\n",
        "\n",
        "    # Find contours in the binary image\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # For each contour, find the bounding rectangle and create a new image\n",
        "    for _, cnt in enumerate(contours):\n",
        "      x, y, w, h = cv2.boundingRect(cnt)\n",
        "      if w*h > min_roi_size:\n",
        "\n",
        "        # add the origin image (so we can later view it)\n",
        "        x_origin.append(image_path)\n",
        "        y_origin.append(image_path)\n",
        "\n",
        "        # lesion type\n",
        "        val_lesion_type.append(lesion_type)\n",
        "\n",
        "        # Add to train set\n",
        "        X_val.append(cv2.resize(img[max(0,y-pad):min(mask.shape[0]-1,y+h+pad),max(0,x-pad):min(mask.shape[1]-1,x+w+pad),:],(256,256)))\n",
        "        y_val.append(cv2.resize(mask[max(0,y-pad):min(mask.shape[0]-1,y+h+pad),max(0,x-pad):min(mask.shape[1]-1,x+w+pad)],(256,256)))\n",
        "\n",
        "  # Convert and Save the data\n",
        "  X_val = np.array(X_val)\n",
        "  y_val = np.array(y_val)\n",
        "  with open(data_folder + 'X_val_with_class.npy', 'wb') as f:\n",
        "    np.save(f, X_val)\n",
        "  with open(data_folder + 'y_val_with_class.npy', 'wb') as f:\n",
        "    np.save(f, y_val)\n",
        "  with open(data_folder + 'val_lesion_type.npy', 'wb') as f:\n",
        "    np.save(f,val_lesion_type)\n"
      ],
      "metadata": {
        "id": "TZEmHZQKws8U"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visulize one image\n",
        "if run_mode == \"cut_pathces\":\n",
        "\n",
        "  index = 100\n",
        "  img = cv2.imread(os.path.join(val_image_dir, x_origin[index]))\n",
        "  mask = cv2.imread(os.path.join(val_mask_dir, y_origin[index]))*255\n",
        "  output = [X_val[index],y_val[index],img,mask]\n",
        "  titles = ['patch','mask patch','original image','original mask']\n",
        "  # Plot couple of examples\n",
        "  fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10,10)) # Define a grid of 2 rows and 3 columns.\n",
        "\n",
        "  for idx, ax in enumerate(axs.flatten()): # axs.flatten() gives us a 1D array to iterate over.\n",
        "      ax.imshow(output[idx], cmap='gray')\n",
        "      ax.set_title(titles[idx])\n",
        "      ax.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8TiXRZ_S8Cub"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Training patches"
      ],
      "metadata": {
        "id": "BjANekWCw88Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode == \"cut_pathces\":\n",
        "\n",
        "  # Define the minimal ROI\n",
        "  min_roi_size = 100\n",
        "  # Create an empty list for validation set\n",
        "  X_train = list()\n",
        "  y_train = list()\n",
        "\n",
        "  # For visulazing the image and mask compared to origin\n",
        "  x_origin = list()\n",
        "  y_origin = list()\n",
        "  train_lesion_type = []\n",
        "\n",
        "  # Train images:\n",
        "  train_image_dir = data_folder +'images/train'\n",
        "  train_mask_dir = data_folder + 'masks/train'\n",
        "  train_list = sorted(os.listdir(train_image_dir))\n",
        "\n",
        "  # Steps:\n",
        "  # 1. take an image\n",
        "  # 2. Cut a window around the mask boundries\n",
        "  # 3. append to whole list\n",
        "\n",
        "  for i,image_path in tqdm(enumerate(train_list)):\n",
        "\n",
        "    # step #1 - read the image and mask\n",
        "    img = cv2.imread(os.path.join(train_image_dir, image_path))\n",
        "\n",
        "    # Assesing the correct masks (0=benign,1=malignant)\n",
        "    lesion_type_index = lookup_table.loc[lookup_table['image_id']== image_path]['pathology']\n",
        "    if lesion_type_index.iloc[0] == 'MALIGNANT':\n",
        "      lesion_type = 1\n",
        "    else:\n",
        "      lesion_type = 0\n",
        "\n",
        "    # Read the mask\n",
        "    mask = cv2.imread(os.path.join(train_mask_dir, image_path),cv2.IMREAD_GRAYSCALE)*255\n",
        "\n",
        "    # Find contours in the binary image\n",
        "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # For each contour, find the bounding rectangle and create a new image\n",
        "    for _, cnt in enumerate(contours):\n",
        "      x, y, w, h = cv2.boundingRect(cnt)\n",
        "      if w*h > min_roi_size:\n",
        "\n",
        "        # add the origin image (so we can later view it)\n",
        "        x_origin.append(image_path)\n",
        "        y_origin.append(image_path)\n",
        "\n",
        "        # lesion type\n",
        "        train_lesion_type.append(lesion_type)\n",
        "\n",
        "        # Add to train set\n",
        "        X_train.append(cv2.resize(img[max(0,y-pad):min(mask.shape[0]-1,y+h+pad),max(0,x-pad):min(mask.shape[1]-1,x+w+pad),:],(256,256)))\n",
        "        y_train.append(cv2.resize(mask[max(0,y-pad):min(mask.shape[0]-1,y+h+pad),max(0,x-pad):min(mask.shape[1]-1,x+w+pad)],(256,256)))\n",
        "\n",
        "  # Convert and Save the data\n",
        "  X_train = np.array(X_train)\n",
        "  y_train = np.array(y_train)\n",
        "  with open(data_folder + 'X_train_with_class.npy', 'wb') as f:\n",
        "    np.save(f, X_train)\n",
        "  with open(data_folder + 'y_train_with_class.npy', 'wb') as f:\n",
        "    np.save(f, y_train)\n",
        "  with open(data_folder + 'train_lesion_type.npy', 'wb') as f:\n",
        "    np.save(f,train_lesion_type)\n"
      ],
      "metadata": {
        "id": "VBZUUAxoCXHe"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visulize one image\n",
        "if run_mode == \"cut_pathces\":\n",
        "\n",
        "  index = 100\n",
        "  img = cv2.imread(os.path.join(train_image_dir, x_origin[index]))\n",
        "  mask = cv2.imread(os.path.join(train_mask_dir, y_origin[index]))*255\n",
        "  output = [X_train[index],y_train[index],img,mask]\n",
        "  titles = ['patch','mask patch','original image','original mask']\n",
        "  # Plot couple of examples\n",
        "  fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(10,10)) # Define a grid of 2 rows and 3 columns.\n",
        "\n",
        "  for idx, ax in enumerate(axs.flatten()): # axs.flatten() gives us a 1D array to iterate over.\n",
        "      ax.imshow(output[idx], cmap='gray')\n",
        "      ax.set_title(titles[idx])\n",
        "      ax.axis('off')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ouMzYhbvMX7n"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 DataLoader"
      ],
      "metadata": {
        "id": "m7nowUUg4Njj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKJz08xq3g63"
      },
      "source": [
        "## 4.1 Define the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "34-vnSLW2Yvz"
      },
      "outputs": [],
      "source": [
        "if run_mode == 'training_model' or run_mode == 'evaluate_model' or run_mode == 'training_classifier_model':\n",
        "\n",
        "  #Assuming image/mask is from shape (256,256,C1/C2)\n",
        "  class CustomDataset(Dataset):\n",
        "      def __init__(self, images, masks,lesion_type, train=True):\n",
        "\n",
        "          self.images = images\n",
        "          self.masks = masks\n",
        "          self.train = train\n",
        "          self.lesion_type = lesion_type\n",
        "\n",
        "        # In training, apply the following transformations\n",
        "          self.train_image_transforms = transforms.Compose([\n",
        "\n",
        "              transforms.Resize((224, 224)), # Resize to fit VGG16 input shape\n",
        "              transforms.RandomHorizontalFlip(),\n",
        "              transforms.RandomVerticalFlip(),\n",
        "              transforms.RandomRotation(45),\n",
        "              transforms.RandomResizedCrop(224),\n",
        "              #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "          ])\n",
        "\n",
        "          # In training, apply the following transformations (including random transformation)\n",
        "          self.train_mask_transforms = transforms.Compose([\n",
        "\n",
        "              transforms.Resize((224, 224)), # Resize to fit VGG16 input shape\n",
        "              transforms.RandomHorizontalFlip(),\n",
        "              transforms.RandomVerticalFlip(),\n",
        "              transforms.RandomRotation(45),\n",
        "              #transforms.RandomResizedCrop(224),\n",
        "              #transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "              transforms.ToTensor()\n",
        "          ])\n",
        "\n",
        "          # In training, apply the following transformations (Excluding random transformation)\n",
        "          self.test_image_transforms = transforms.Compose([\n",
        "              transforms.Resize((224, 224)),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "          ] )\n",
        "\n",
        "          # In testing, apply the following transformations (Excluding random transformation)\n",
        "          self.test_mask_transforms = transforms.Compose([\n",
        "              transforms.Resize((224, 224)),\n",
        "              transforms.ToTensor()\n",
        "          ])\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.images)\n",
        "\n",
        "      def __getitem__(self, idx):\n",
        "\n",
        "          # Get the image index\n",
        "          #image = self.images[idx]*255\n",
        "          image = self.images[idx]\n",
        "\n",
        "          # Convert the numpy array to a PIL Image\n",
        "          image = Image.fromarray(image.astype('uint8'))\n",
        "\n",
        "          # Get the corresponding mask index (need to reshape to (256,256) to convert to PIL later\n",
        "          #mask = self.masks[idx] *255\n",
        "          mask = self.masks[idx]\n",
        "\n",
        "          lesion_type = self.lesion_type[idx]\n",
        "\n",
        "          # convert to fit to PIL image (256,256,1)-> (256,256)\n",
        "          #mask = mask.squeeze()\n",
        "\n",
        "          # convert to PIL image (not before updating from [0,1] to [0,255]), it will be normalized later\n",
        "          mask = Image.fromarray(mask.astype('uint8'))\n",
        "\n",
        "          if self.train:\n",
        "\n",
        "              # make a seed with numpy generator\n",
        "              seed = np.random.randint(2147483647)\n",
        "              random.seed(seed)\n",
        "              torch.manual_seed(seed)\n",
        "              image = self.train_image_transforms(image)\n",
        "\n",
        "              random.seed(seed)\n",
        "              torch.manual_seed(seed)\n",
        "              mask = self.train_mask_transforms(mask)\n",
        "\n",
        "          else:\n",
        "              image = self.test_image_transforms(image)\n",
        "              mask = self.test_mask_transforms(mask)\n",
        "\n",
        "          return image, mask, lesion_type\n",
        "\n",
        "\n",
        "  # Load X and y from pickle files\n",
        "\n",
        "  X_train = np.load(data_folder + 'X_train_with_class.npy')\n",
        "  y_train = np.load(data_folder + 'y_train_with_class.npy')\n",
        "  train_lesion_type = np.load(data_folder + 'train_lesion_type.npy')\n",
        "  X_val = np.load(data_folder + 'X_val_with_class.npy')\n",
        "  y_val = np.load(data_folder + 'y_val_with_class.npy')\n",
        "  val_lesion_type = np.load(data_folder + 'val_lesion_type.npy')\n",
        "\n",
        "\n",
        "  train_dataset = CustomDataset(X_train, y_train,train_lesion_type,train=True)\n",
        "  val_dataset = CustomDataset(X_val, y_val,val_lesion_type, train=False)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Test the dataloader"
      ],
      "metadata": {
        "id": "9xJ0NMXX4DK_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "gnVWNQYpkL25"
      },
      "outputs": [],
      "source": [
        "if run_mode == 'testing_dataloader':\n",
        "\n",
        "  index= 23\n",
        "  # define the transform\n",
        "  train_image_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)), # Resize to fit VGG16 input shape\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # VGG16 normalization\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(45),\n",
        "\n",
        "  ])\n",
        "  X_train = np.load(data_folder + 'X_train_with_class.npy')\n",
        "  y_train = np.load(data_folder + 'y_train_with_class.npy')\n",
        "  train_lesion_type = np.load(data_folder + 'train_lesion_type.npy')\n",
        "  # Load the image and convert to PIL image\n",
        "  # Convert array to Image\n",
        "  #image= X[0]*255\n",
        "  image= X_train[index]\n",
        "  img = Image.fromarray(image.astype('uint8'))\n",
        "  img2= img\n",
        "\n",
        "  # make a seed with numpy generator\n",
        "  seed = np.random.randint(2147483647)\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  img = train_image_transforms(img)\n",
        "\n",
        "  # Assuming your DataLoader returns a batch of images and labels\n",
        "  # we will display the first image from the batch\n",
        "\n",
        "  def unnormalize(tensor):\n",
        "      mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "      std = torch.tensor([0.229, 0.224, 0.225])\n",
        "      for t, m, s in zip(tensor, mean, std):\n",
        "          t.mul_(s).add_(m)\n",
        "      return tensor\n",
        "\n",
        "  img = unnormalize(img)\n",
        "  img = transforms.ToPILImage()(img).convert(\"RGB\")\n",
        "\n",
        "\n",
        "  # In training, apply the following transformations (including random transformation)\n",
        "  train_mask_transforms = transforms.Compose([\n",
        "\n",
        "      transforms.Resize((256, 256)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.RandomHorizontalFlip(), # todo - return back\n",
        "      transforms.RandomVerticalFlip(),\n",
        "      transforms.RandomRotation(100),\n",
        "  ])\n",
        "\n",
        "  # Convert array to Image\n",
        "  #mask1= y[0] *255\n",
        "  mask1= y_train[index]\n",
        "  mask1 = mask1.reshape((256,256))\n",
        "  mask = Image.fromarray(mask1.astype('uint8'))\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  mask = train_mask_transforms(mask)\n",
        "  mask = transforms.ToPILImage()(mask).convert(\"RGB\")\n",
        "\n",
        "\n",
        "  # Create subplots\n",
        "  fig, axs = plt.subplots(1, 4, figsize=(10, 5))  # adjust the size as needed\n",
        "\n",
        "  # Show the images\n",
        "  axs[0].imshow(img)\n",
        "  axs[0].axis('off')  # to hide the axis\n",
        "  axs[0].set_title('Image 1')\n",
        "\n",
        "  axs[1].imshow(img2)\n",
        "  axs[1].axis('off')  # to hide the axis\n",
        "  axs[1].set_title('Image 2')\n",
        "\n",
        "  axs[2].imshow(mask)\n",
        "  axs[2].axis('off')  # to hide the axis\n",
        "  axs[2].set_title('Mask')\n",
        "\n",
        "  axs[3].imshow((np.array(mask))*img)\n",
        "  axs[3].axis('off')  # to hide the axis\n",
        "  axs[3].set_title('Image with Mask')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  print(train_lesion_type[index])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Define stop and update learning rate conditions"
      ],
      "metadata": {
        "id": "bVK5NZjB1ra-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "yHLFuviKHAY8"
      },
      "outputs": [],
      "source": [
        "if run_mode == 'training_model' or run_mode == 'training_classifier_model':\n",
        "  NUM_OF_EPOCHS_TO_REDUCE_LR = 30\n",
        "  NUM_OF_EPOCHS_TO_STOP = 50\n",
        "\n",
        "\n",
        "  def model_checkpoint(model,valid_loss,best_val_loss,epoch,best_epoch,lr,last_lr_update,stop_training=False,model_option='patch'):\n",
        "    if valid_loss < best_val_loss:\n",
        "\n",
        "      # Imporvement in current validation loss, udpate best epoch\n",
        "      print(f\"Validation was improved, updated best loss: {valid_loss},best_epoch:{epoch},lr:{lr}\\n\")\n",
        "      best_val_loss = valid_loss\n",
        "      best_epoch = epoch\n",
        "      last_lr_update = epoch\n",
        "\n",
        "      # Save the model\n",
        "      if model_option == 'patch':\n",
        "        torch.save(model.state_dict(), models_folder + 'Patch_model_padding.pth')\n",
        "      else:\n",
        "        torch.save(model.state_dict(), models_folder + 'classifier_model_padding.pth')\n",
        "\n",
        "    # Stop training if the validation loss doesn't improve for 10 epochs\n",
        "    else:\n",
        "      if epoch - best_epoch >= NUM_OF_EPOCHS_TO_STOP:\n",
        "        print(f\"No improvement in validation loss for {NUM_OF_EPOCHS_TO_STOP} epochs, stopping.\")\n",
        "        stop_training = True\n",
        "\n",
        "      # If need only to reduce learning rate\n",
        "      elif epoch - last_lr_update >= NUM_OF_EPOCHS_TO_REDUCE_LR:\n",
        "        last_lr_update = epoch\n",
        "        lr/=10\n",
        "        print(f\"No improvement in validation loss for {NUM_OF_EPOCHS_TO_REDUCE_LR} epochs, reducing learning rate to {lr}.\\n\")\n",
        "\n",
        "    return stop_training,best_epoch,best_val_loss,lr,last_lr_update\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Classification model training"
      ],
      "metadata": {
        "id": "RliRFUorMy8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Define the model"
      ],
      "metadata": {
        "id": "XHHkMTRWM5Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode == 'training_classifier_model':\n",
        "\n",
        "  class Classifer_model(nn.Module):\n",
        "      def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained VGG16 model and adjust the final layer\n",
        "        self.classifer_model = models.vgg16(pretrained=True)\n",
        "\n",
        "        #print(models.vgg16_bn(pretrained=True))\n",
        "\n",
        "        # Change the final layer of VGG16 Model for Transfer Learning\n",
        "        self.classifer_model.classifier[6] = torch.nn.Sequential(\n",
        "            torch.nn.Linear(4096, 256),\n",
        "            torch.nn.BatchNorm1d(256),  # Batch Normalization layer\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.4),\n",
        "            torch.nn.Linear(256, 2), # Since we have two classes\n",
        "            torch.nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
        "          )\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "\n",
        "        x = self.classifer_model(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "P7G1L4ClNbtS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Train"
      ],
      "metadata": {
        "id": "JhZrlKlgNcW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if run_mode == 'training_classifier_model':\n",
        "\n",
        "  # Specify device\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  # Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
        "  print(f'Working on device={device}')\n",
        "\n",
        "  # Initialize the patch model\n",
        "  '''patch_model = VGGUnet(models.vgg16_bn, pretrained=True, out_channels=1)\n",
        "  patch_model.load_state_dict(torch.load(models_folder + 'Patch_model_padding.pth'))\n",
        "  patch_model.to(device)\n",
        "  patch_model.eval()'''\n",
        "\n",
        "  # Learning rate at start\n",
        "  lr = 0.00001\n",
        "\n",
        "  # choose a suitable value\n",
        "  n_epochs = 1000\n",
        "\n",
        "  # Define the model\n",
        "  #classifer_model=  Classifer_model().to(device)\n",
        "\n",
        "  classifer_model=  Classifer_model().to(device)\n",
        "  #classifer_model= LesionClassifier().to(device)\n",
        "\n",
        "  # Define the optimizer\n",
        "  optimizer = optim.Adam(classifer_model.parameters(), lr=lr)\n",
        "\n",
        "  # Define Loss Function and Optimizer\n",
        "  criterion = torch.nn.NLLLoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "EsGCgWtAQ44R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9292ff-0ac1-42d0-a2df-b9a7e91513bd"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Working on device=cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 259MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3  Metrics"
      ],
      "metadata": {
        "id": "6beXJ1iZdqhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, training_loader, valid_loader, n_epochs,lr,batch_size):\n",
        "\n",
        "    # Init all values\n",
        "    best_epoch = 1\n",
        "    valid_loss = 0\n",
        "    best_val_loss = float('inf')\n",
        "    last_lr_update = best_epoch\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        classifer_model.train()\n",
        "\n",
        "        train_loss = 0\n",
        "        train_corrects = 0\n",
        "        for images, masks,lesion_type in training_loader:\n",
        "\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            lesion_type = lesion_type.to(device)\n",
        "\n",
        "            output = classifer_model(images)\n",
        "            loss = criterion(output,lesion_type)\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            _, preds = torch.max(output, 1)\n",
        "            train_corrects += torch.sum(preds == lesion_type.data)\n",
        "\n",
        "        classifer_model.eval()\n",
        "        valid_loss = 0\n",
        "        val_corrects = 0\n",
        "        with torch.no_grad():\n",
        "\n",
        "            for images, masks, lesion_type in valid_loader:\n",
        "                images = images.to(device)\n",
        "                masks = masks.to(device)\n",
        "                lesion_type = lesion_type.to(device)\n",
        "\n",
        "\n",
        "                output = classifer_model(images)\n",
        "                loss = criterion(output,lesion_type)\n",
        "                valid_loss += loss.item() * images.size(0)\n",
        "\n",
        "                _, preds = torch.max(output, 1)\n",
        "                val_corrects += torch.sum(preds == lesion_type.data)\n",
        "\n",
        "        train_loss = train_loss / len(y_train)\n",
        "        valid_loss = valid_loss / len(y_val)\n",
        "\n",
        "        train_acc = train_corrects.double() / len(y_train)\n",
        "        valid_acc = val_corrects.double() / len(y_val)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{n_epochs}, Training Loss: {train_loss}, Validation Loss: {valid_loss}, Training Acc: {train_acc}, Validation Acc: {valid_acc}')\n",
        "\n",
        "        stop_training,best_epoch,best_val_loss,lr,last_lr_update = model_checkpoint(classifer_model,valid_loss,best_val_loss,epoch,best_epoch,lr,last_lr_update,model_option='Classifier')\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] = lr\n",
        "\n",
        "        if stop_training:\n",
        "          break\n",
        "\n",
        "# Train the model\n",
        "train_model(classifer_model, train_loader, val_loader, n_epochs,lr,batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsZ9g9joOpb5",
        "outputId": "c5f7fda8-b000-4fae-aab9-822240dd0274"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000, Training Loss: 0.7685163025280236, Validation Loss: 0.669121546084892, Training Acc: 0.492379835873388, Validation Acc: 0.5821596244131455\n",
            "Validation was improved, updated best loss: 0.669121546084892,best_epoch:0,lr:1e-05\n",
            "\n",
            "Epoch 2/1000, Training Loss: 0.7588874988371435, Validation Loss: 0.6558456927398001, Training Acc: 0.5076201641266119, Validation Acc: 0.6338028169014085\n",
            "Validation was improved, updated best loss: 0.6558456927398001,best_epoch:1,lr:1e-05\n",
            "\n",
            "Epoch 3/1000, Training Loss: 0.7409832239989519, Validation Loss: 0.6730013081165547, Training Acc: 0.533411488862837, Validation Acc: 0.568075117370892\n",
            "Epoch 4/1000, Training Loss: 0.7253154646190362, Validation Loss: 0.6368252850474326, Training Acc: 0.52989449003517, Validation Acc: 0.6572769953051644\n",
            "Validation was improved, updated best loss: 0.6368252850474326,best_epoch:3,lr:1e-05\n",
            "\n",
            "Epoch 5/1000, Training Loss: 0.6904013897581648, Validation Loss: 0.6336188086881324, Training Acc: 0.5873388042203985, Validation Acc: 0.6525821596244131\n",
            "Validation was improved, updated best loss: 0.6336188086881324,best_epoch:4,lr:1e-05\n",
            "\n",
            "Epoch 6/1000, Training Loss: 0.6837965317096688, Validation Loss: 0.6053466477864226, Training Acc: 0.5955451348182883, Validation Acc: 0.6948356807511737\n",
            "Validation was improved, updated best loss: 0.6053466477864226,best_epoch:5,lr:1e-05\n",
            "\n",
            "Epoch 7/1000, Training Loss: 0.6690780343651632, Validation Loss: 0.5729646791874523, Training Acc: 0.6178194607268463, Validation Acc: 0.6901408450704225\n",
            "Validation was improved, updated best loss: 0.5729646791874523,best_epoch:6,lr:1e-05\n",
            "\n",
            "Epoch 8/1000, Training Loss: 0.6556940072446468, Validation Loss: 0.5525581057362713, Training Acc: 0.6213364595545134, Validation Acc: 0.7042253521126761\n",
            "Validation was improved, updated best loss: 0.5525581057362713,best_epoch:7,lr:1e-05\n",
            "\n",
            "Epoch 9/1000, Training Loss: 0.6524278652206814, Validation Loss: 0.6005529684360038, Training Acc: 0.6307151230949589, Validation Acc: 0.6384976525821596\n",
            "Epoch 10/1000, Training Loss: 0.6283085295212087, Validation Loss: 0.523291290925702, Training Acc: 0.6518171160609613, Validation Acc: 0.7323943661971831\n",
            "Validation was improved, updated best loss: 0.523291290925702,best_epoch:9,lr:1e-05\n",
            "\n",
            "Epoch 11/1000, Training Loss: 0.6375678110374235, Validation Loss: 0.5156156245811444, Training Acc: 0.6412661195779601, Validation Acc: 0.7370892018779343\n",
            "Validation was improved, updated best loss: 0.5156156245811444,best_epoch:10,lr:1e-05\n",
            "\n",
            "Epoch 12/1000, Training Loss: 0.6371120858723505, Validation Loss: 0.5042312439058868, Training Acc: 0.6518171160609613, Validation Acc: 0.7417840375586855\n",
            "Validation was improved, updated best loss: 0.5042312439058868,best_epoch:11,lr:1e-05\n",
            "\n",
            "Epoch 13/1000, Training Loss: 0.61044323035589, Validation Loss: 0.56353708024316, Training Acc: 0.7045720984759671, Validation Acc: 0.6384976525821596\n",
            "Epoch 14/1000, Training Loss: 0.6123624252546295, Validation Loss: 0.5131104415011518, Training Acc: 0.678780773739742, Validation Acc: 0.7370892018779343\n",
            "Epoch 15/1000, Training Loss: 0.6068998539573562, Validation Loss: 0.5060993880733078, Training Acc: 0.694021101992966, Validation Acc: 0.727699530516432\n",
            "Epoch 16/1000, Training Loss: 0.6118921377594557, Validation Loss: 0.5343224497188425, Training Acc: 0.6811254396248534, Validation Acc: 0.7464788732394366\n",
            "Epoch 17/1000, Training Loss: 0.5745160185228618, Validation Loss: 0.4758810546476516, Training Acc: 0.6975381008206331, Validation Acc: 0.8028169014084507\n",
            "Validation was improved, updated best loss: 0.4758810546476516,best_epoch:16,lr:1e-05\n",
            "\n",
            "Epoch 18/1000, Training Loss: 0.5738467052840963, Validation Loss: 0.5086877246697744, Training Acc: 0.7139507620164126, Validation Acc: 0.7605633802816901\n",
            "Epoch 19/1000, Training Loss: 0.5600192480461819, Validation Loss: 0.49924166157771727, Training Acc: 0.7280187573270809, Validation Acc: 0.7417840375586855\n",
            "Epoch 20/1000, Training Loss: 0.5641577692411989, Validation Loss: 0.4667610486908138, Training Acc: 0.7315357561547479, Validation Acc: 0.7699530516431925\n",
            "Validation was improved, updated best loss: 0.4667610486908138,best_epoch:19,lr:1e-05\n",
            "\n",
            "Epoch 21/1000, Training Loss: 0.5476131322515249, Validation Loss: 0.5670873567531926, Training Acc: 0.7080890973036342, Validation Acc: 0.727699530516432\n",
            "Epoch 22/1000, Training Loss: 0.5749346766913482, Validation Loss: 0.46478491755718354, Training Acc: 0.7151230949589683, Validation Acc: 0.7981220657276995\n",
            "Validation was improved, updated best loss: 0.46478491755718354,best_epoch:21,lr:1e-05\n",
            "\n",
            "Epoch 23/1000, Training Loss: 0.5456568386322728, Validation Loss: 0.4627109168560852, Training Acc: 0.7409144196951933, Validation Acc: 0.7981220657276995\n",
            "Validation was improved, updated best loss: 0.4627109168560852,best_epoch:22,lr:1e-05\n",
            "\n",
            "Epoch 24/1000, Training Loss: 0.5755478359610366, Validation Loss: 0.480880168402139, Training Acc: 0.7022274325908557, Validation Acc: 0.7652582159624414\n",
            "Epoch 25/1000, Training Loss: 0.5202689095370795, Validation Loss: 0.44557502599949006, Training Acc: 0.7444314185228604, Validation Acc: 0.7981220657276995\n",
            "Validation was improved, updated best loss: 0.44557502599949006,best_epoch:24,lr:1e-05\n",
            "\n",
            "Epoch 26/1000, Training Loss: 0.5303564885017601, Validation Loss: 0.46183187608987514, Training Acc: 0.7491207502930832, Validation Acc: 0.7981220657276995\n",
            "Epoch 27/1000, Training Loss: 0.5401445747400084, Validation Loss: 0.5022276484630477, Training Acc: 0.7549824150058616, Validation Acc: 0.755868544600939\n",
            "Epoch 28/1000, Training Loss: 0.5170527706042823, Validation Loss: 0.5288747729549945, Training Acc: 0.7432590855803047, Validation Acc: 0.7042253521126761\n",
            "Epoch 29/1000, Training Loss: 0.5447750169814402, Validation Loss: 0.5087909439639866, Training Acc: 0.738569753810082, Validation Acc: 0.7793427230046949\n",
            "Epoch 30/1000, Training Loss: 0.5177764381082228, Validation Loss: 0.4813391623642523, Training Acc: 0.7538100820633059, Validation Acc: 0.7652582159624414\n",
            "Epoch 31/1000, Training Loss: 0.5508639357573542, Validation Loss: 0.46819808673410906, Training Acc: 0.7467760844079718, Validation Acc: 0.7981220657276995\n",
            "Epoch 32/1000, Training Loss: 0.5048771768214415, Validation Loss: 0.4649243517101091, Training Acc: 0.7538100820633059, Validation Acc: 0.7981220657276995\n",
            "Epoch 33/1000, Training Loss: 0.5016542804478761, Validation Loss: 0.4750411073926469, Training Acc: 0.7655334114888628, Validation Acc: 0.7887323943661972\n",
            "Epoch 34/1000, Training Loss: 0.5347465159675021, Validation Loss: 0.5003963300319905, Training Acc: 0.7561547479484173, Validation Acc: 0.7605633802816901\n",
            "Epoch 35/1000, Training Loss: 0.5142174915509934, Validation Loss: 0.4393437234728549, Training Acc: 0.7655334114888628, Validation Acc: 0.7934272300469484\n",
            "Validation was improved, updated best loss: 0.4393437234728549,best_epoch:34,lr:1e-05\n",
            "\n",
            "Epoch 36/1000, Training Loss: 0.48508360433410785, Validation Loss: 0.4690281978235558, Training Acc: 0.7620164126611957, Validation Acc: 0.7699530516431925\n",
            "Epoch 37/1000, Training Loss: 0.5003237910874424, Validation Loss: 0.5723577513941017, Training Acc: 0.7643610785463071, Validation Acc: 0.727699530516432\n",
            "Epoch 38/1000, Training Loss: 0.49989505204757523, Validation Loss: 0.5070354429209176, Training Acc: 0.7690504103165299, Validation Acc: 0.755868544600939\n",
            "Epoch 39/1000, Training Loss: 0.5164914652443156, Validation Loss: 0.49593408286851337, Training Acc: 0.7584994138335287, Validation Acc: 0.7887323943661972\n",
            "Epoch 40/1000, Training Loss: 0.48320194914640047, Validation Loss: 0.4932430907034538, Training Acc: 0.7784290738569754, Validation Acc: 0.7605633802816901\n",
            "Epoch 41/1000, Training Loss: 0.5316870515120127, Validation Loss: 0.4614061052810418, Training Acc: 0.738569753810082, Validation Acc: 0.784037558685446\n",
            "Epoch 42/1000, Training Loss: 0.48331434593250994, Validation Loss: 0.47074746427961356, Training Acc: 0.7924970691676435, Validation Acc: 0.7793427230046949\n",
            "Epoch 43/1000, Training Loss: 0.49052514633707534, Validation Loss: 0.48010479732298517, Training Acc: 0.7678780773739742, Validation Acc: 0.7605633802816901\n",
            "Epoch 44/1000, Training Loss: 0.4767977403867706, Validation Loss: 0.4657457087521262, Training Acc: 0.7784290738569754, Validation Acc: 0.8122065727699531\n",
            "Epoch 45/1000, Training Loss: 0.5150801471640888, Validation Loss: 0.43971724073651813, Training Acc: 0.7444314185228604, Validation Acc: 0.7981220657276995\n",
            "Epoch 46/1000, Training Loss: 0.5101415837251287, Validation Loss: 0.4647173349846137, Training Acc: 0.7397420867526378, Validation Acc: 0.7934272300469484\n",
            "Epoch 47/1000, Training Loss: 0.4816264407785668, Validation Loss: 0.4716940701287677, Training Acc: 0.7889800703399765, Validation Acc: 0.7793427230046949\n",
            "Epoch 48/1000, Training Loss: 0.5027308252324533, Validation Loss: 0.4874941596122975, Training Acc: 0.76084407971864, Validation Acc: 0.7746478873239436\n",
            "Epoch 49/1000, Training Loss: 0.49347025058194033, Validation Loss: 0.5227992207231656, Training Acc: 0.7725674091441969, Validation Acc: 0.7699530516431925\n",
            "Epoch 50/1000, Training Loss: 0.49111312781520633, Validation Loss: 0.4823856486680922, Training Acc: 0.76084407971864, Validation Acc: 0.7934272300469484\n",
            "Epoch 51/1000, Training Loss: 0.4720771511512233, Validation Loss: 0.48513815268664295, Training Acc: 0.7737397420867526, Validation Acc: 0.7793427230046949\n",
            "Epoch 52/1000, Training Loss: 0.4796758911813689, Validation Loss: 0.5162390033963701, Training Acc: 0.7842907385697537, Validation Acc: 0.7370892018779343\n",
            "Epoch 53/1000, Training Loss: 0.48664511263300636, Validation Loss: 0.4700774910024634, Training Acc: 0.7643610785463071, Validation Acc: 0.7793427230046949\n",
            "Epoch 54/1000, Training Loss: 0.48712010456277505, Validation Loss: 0.4850654551680659, Training Acc: 0.7620164126611957, Validation Acc: 0.7934272300469484\n",
            "Epoch 55/1000, Training Loss: 0.4794596060499077, Validation Loss: 0.5575149641350401, Training Acc: 0.783118405627198, Validation Acc: 0.7464788732394366\n",
            "Epoch 56/1000, Training Loss: 0.4568989185173095, Validation Loss: 0.5460389578286471, Training Acc: 0.7749120750293083, Validation Acc: 0.7417840375586855\n",
            "Epoch 57/1000, Training Loss: 0.4518281293013171, Validation Loss: 0.5285235282400964, Training Acc: 0.8030480656506447, Validation Acc: 0.7605633802816901\n",
            "Epoch 58/1000, Training Loss: 0.45410448621089405, Validation Loss: 0.5678646838720975, Training Acc: 0.7784290738569754, Validation Acc: 0.755868544600939\n",
            "Epoch 59/1000, Training Loss: 0.4631443493950409, Validation Loss: 0.6657649398969373, Training Acc: 0.783118405627198, Validation Acc: 0.7136150234741784\n",
            "Epoch 60/1000, Training Loss: 0.49105454413580585, Validation Loss: 0.5732530601707423, Training Acc: 0.779601406799531, Validation Acc: 0.7511737089201878\n",
            "Epoch 61/1000, Training Loss: 0.4928479952801015, Validation Loss: 0.6164361462346825, Training Acc: 0.7702227432590856, Validation Acc: 0.7417840375586855\n",
            "Epoch 62/1000, Training Loss: 0.4482987589810965, Validation Loss: 0.5143646225683006, Training Acc: 0.798358733880422, Validation Acc: 0.784037558685446\n",
            "Epoch 63/1000, Training Loss: 0.4573459692412915, Validation Loss: 0.5291418111380277, Training Acc: 0.7913247362250879, Validation Acc: 0.7934272300469484\n",
            "Epoch 64/1000, Training Loss: 0.4738542777851905, Validation Loss: 0.5724800881085821, Training Acc: 0.7749120750293083, Validation Acc: 0.7464788732394366\n",
            "Epoch 65/1000, Training Loss: 0.45742230209907364, Validation Loss: 0.5237811356083328, Training Acc: 0.7807737397420867, Validation Acc: 0.8075117370892019\n",
            "No improvement in validation loss for 30 epochs, reducing learning rate to 1.0000000000000002e-06.\n",
            "\n",
            "Epoch 66/1000, Training Loss: 0.45141441803884114, Validation Loss: 0.5526757077991683, Training Acc: 0.7878077373974208, Validation Acc: 0.7464788732394366\n",
            "Epoch 67/1000, Training Loss: 0.47070542475752364, Validation Loss: 0.5992410104599357, Training Acc: 0.7819460726846423, Validation Acc: 0.7417840375586855\n",
            "Epoch 68/1000, Training Loss: 0.4610303768239575, Validation Loss: 0.5233247666291787, Training Acc: 0.7749120750293083, Validation Acc: 0.7605633802816901\n",
            "Epoch 69/1000, Training Loss: 0.4454259665602117, Validation Loss: 0.5746259090486266, Training Acc: 0.7995310668229777, Validation Acc: 0.7511737089201878\n",
            "Epoch 70/1000, Training Loss: 0.45700567904837947, Validation Loss: 0.5335506184000365, Training Acc: 0.7866354044548651, Validation Acc: 0.7605633802816901\n",
            "Epoch 71/1000, Training Loss: 0.45279411005526604, Validation Loss: 0.54161652656788, Training Acc: 0.8124267291910902, Validation Acc: 0.7652582159624414\n",
            "Epoch 72/1000, Training Loss: 0.45876766117628126, Validation Loss: 0.5806132146450276, Training Acc: 0.7854630715123094, Validation Acc: 0.755868544600939\n",
            "Epoch 73/1000, Training Loss: 0.4356714773918791, Validation Loss: 0.5891394939780795, Training Acc: 0.8065650644783118, Validation Acc: 0.7464788732394366\n",
            "Epoch 74/1000, Training Loss: 0.46205830315492635, Validation Loss: 0.561470692146552, Training Acc: 0.8077373974208675, Validation Acc: 0.7605633802816901\n",
            "Epoch 75/1000, Training Loss: 0.42749466340888426, Validation Loss: 0.542897836143422, Training Acc: 0.8089097303634232, Validation Acc: 0.755868544600939\n",
            "Epoch 76/1000, Training Loss: 0.4241003772158695, Validation Loss: 0.570283079371206, Training Acc: 0.7960140679953106, Validation Acc: 0.7652582159624414\n",
            "Epoch 77/1000, Training Loss: 0.41039842849320013, Validation Loss: 0.5740175655750042, Training Acc: 0.8159437280187573, Validation Acc: 0.755868544600939\n",
            "Epoch 78/1000, Training Loss: 0.4238910867907937, Validation Loss: 0.5465491948553094, Training Acc: 0.8065650644783118, Validation Acc: 0.7652582159624414\n",
            "Epoch 79/1000, Training Loss: 0.40878310698277787, Validation Loss: 0.5543607271892924, Training Acc: 0.8300117233294255, Validation Acc: 0.7652582159624414\n",
            "Epoch 80/1000, Training Loss: 0.4551292793412561, Validation Loss: 0.5435806221805268, Training Acc: 0.776084407971864, Validation Acc: 0.7652582159624414\n",
            "Epoch 81/1000, Training Loss: 0.43503264651348833, Validation Loss: 0.5385993780664435, Training Acc: 0.8007033997655334, Validation Acc: 0.7605633802816901\n",
            "Epoch 82/1000, Training Loss: 0.44429379009837144, Validation Loss: 0.5401866945302543, Training Acc: 0.798358733880422, Validation Acc: 0.784037558685446\n",
            "Epoch 83/1000, Training Loss: 0.4210311103468064, Validation Loss: 0.53220465709346, Training Acc: 0.8089097303634232, Validation Acc: 0.7652582159624414\n",
            "Epoch 84/1000, Training Loss: 0.4426463954369041, Validation Loss: 0.5631062324058282, Training Acc: 0.7948417350527549, Validation Acc: 0.755868544600939\n",
            "Epoch 85/1000, Training Loss: 0.402158965259476, Validation Loss: 0.569244812911665, Training Acc: 0.8182883939038686, Validation Acc: 0.7605633802816901\n",
            "No improvement in validation loss for 50 epochs, stopping.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "G6CpJFMJ92DD",
        "bBMEfoC597D7"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}